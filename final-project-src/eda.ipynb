{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ccf486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13749, 164)\n"
     ]
    }
   ],
   "source": [
    "import pyreadstat\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"ATP W89.sav\"\n",
    "df, meta = pyreadstat.read_sav(file_path)\n",
    "# print(df.shape) # 164 columns\n",
    "\n",
    "# drop clearly irrelevant\n",
    "# only need columns with personal/demographic data, useful social media habits, target\n",
    "# drop other survey response columns\n",
    "\n",
    "# print(df.columns)\n",
    "df = df.drop(columns=[\"QKEY\"])\n",
    "interview_data = [x for x in df.columns if (\"INTERVIEW\" in x or \"DEVICE\" in x or \"FORM\" in x)] # asked ChatGPT for example subsetting list based on element values\n",
    "df = df.drop(columns=interview_data)\n",
    "\n",
    "# our target: how engaged individuals were\n",
    "target = \"ENGACTCT_W89\"\n",
    "# 2 features about social media habits selected from examining meaning in questionnaire/their metadata labels\n",
    "sm_features = [\"CLIMSEEK_W89\",\"FOLCLIM_W89\"]\n",
    "\n",
    "# no longer need other questionnaire data, only using ENGACTCT to measure engagement, two social media habit cols relevant, personal data\n",
    "personal_data = [x for x in df.columns if (\"W89\" not in x)]\n",
    "\n",
    "keep_columns = personal_data + sm_features + [target]\n",
    "df = df.loc[:,keep_columns]\n",
    "# print(df.columns)\n",
    "\n",
    "# using PCA to determine PCs, most used features in those PCs, dropping others\n",
    "# need at least 100 instances, drop those where don't have value for any of selected features\n",
    "# modified range, examined few columns at time, view labels to see what data values contained\n",
    "# for i in range(30,31):\n",
    "#     print(df[df.columns[i]].value_counts())\n",
    "#     print(meta.variable_value_labels.get(df.columns[i]))\n",
    "\n",
    "def get_na_columns():\n",
    "    na_counts = pd.DataFrame({\"na\":df.isna().apply(sum)})\n",
    "    na_columns = na_counts.loc[na_counts[\"na\"]>0]\n",
    "    return na_columns\n",
    "\n",
    "df = df.loc[df[\"CLIMSEEK_W89\"].isna()==False]\n",
    "df = df.loc[df[\"FOLCLIM_W89\"].isna()==False]\n",
    "\n",
    "# print(get_na_columns())\n",
    "# na_columns = get_na_columns()\n",
    "# for col in na_columns.index:\n",
    "#     print(df[col].value_counts())\n",
    "#     print(meta.variable_value_labels.get(col))\n",
    "df = df.drop(columns=[\"F_BORN\",\"F_PARTYLN_FINAL\"]) # dropped these columns b/c high proportion of NaNs & exist at least 1 other complete col describing similar data\n",
    "\n",
    "# print(get_na_columns()) # other cols have unique, useful info and few enough NaNs to impute\n",
    "\n",
    "# impute missing vals with mode\n",
    "impute_cols = get_na_columns().index\n",
    "for i in range(len(impute_cols)):\n",
    "    column = impute_cols[i]\n",
    "    col_mode = df[column].mode()[0]\n",
    "    df[column] = df[column].fillna(col_mode)\n",
    "    df[column] = df[column].apply(lambda x: col_mode if x==99.0 else x) # treat 99.0 (\"Refused\") as missing val\n",
    "    # print(df[column].value_counts()) # verify no 99.0s remain\n",
    "\n",
    "# print(get_na_columns())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
